# Stochastic_Optimization_A.D.A.M.

## Description
This machine learning project is intended to teach how the Gradient Descent (GD), Stochastic Gradient Descent (SGD), and the ADAM optimizer work. I used a self-generated dataset to show convergence rates for SGD, GD, and ADAM. I learned how to add and manipulate hyper parameters such as the learning rate, batch size, and training size.
